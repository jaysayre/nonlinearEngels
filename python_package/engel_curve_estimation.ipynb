{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "engel_curve_estimation.py -- contact: Jay Sayre, jsayre@ucdavis.edu\n",
    "\n",
    "Python code to replicate the nonlinear price index and welfare estimation procedure in Atkin, Faber, Fally and Gonzalez-Navarro (2023).\n",
    "\n",
    "- Step 1: Estimate the engel curves, first by generating smoothed income for each market-period, and then by generating smoothed expenditures for each market-period-good\n",
    "- Step 2: Estimate various measures of welfare\n",
    "- Step 3: Generate plots for welfare measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.interpolate import interp1d, CubicSpline\n",
    "import matplotlib.pyplot as plt\n",
    "from engel_curves import *\n",
    "\n",
    "### Directories (change these to your own)\n",
    "base_dir                       =  os.path.join(os.path.expanduser(\"~\"),\"Dropbox\",\"Projects\",\"Engel_Ado\") \n",
    "data_dir                       =  os.path.join(base_dir,\"data\",\"source_data\")\n",
    "output_dir                     =  os.path.join(base_dir,\"output\")\n",
    "temp_dir                       =  os.path.join(output_dir,\"temp\")\n",
    "plot_dir                       =  os.path.join(output_dir,\"plots\")\n",
    "\n",
    "### Inputs ###\n",
    "cons_data                      =  os.path.join(data_dir,\"basefile.dta\")\n",
    "price_data                     =  os.path.join(data_dir,\"cleaned_prices.dta\") ### For exact price correction\n",
    "for_1stord_data                =  os.path.join(temp_dir,\"fir_ord_data.dta\") ### For first order approximation\n",
    "\n",
    "### Intermediates ###\n",
    "smth_x_shares_dta              =  os.path.join(temp_dir,\"smoothexp_shares_python.dta\")\n",
    "\n",
    "### Outputs ###\n",
    "price_indices_dta              =  os.path.join(output_dir,\"price_indices_python.dta\")\n",
    "\n",
    "### Parameters ###\n",
    "evaluation_points              =  100       ### Write 100 for 101 evaluation points (incl. zero), 1000 for 1001, etc.\n",
    "bandwidth_portion              =  0.25\n",
    "tails_extrapolation_percentage =  0.05\n",
    "infinity_stata                 =  99999999  ### Stata doesn't like np.inf, so store as large-ish number\n",
    "sigma                          =  0.7       ### Price elasticity (sigma=0.7 in AFFG baseline calibration)\n",
    "\n",
    "winsorize                      =  True      ### If you want to winsorize large outlay values\n",
    "extrapolate_tails              =  True      ### Whether to always extrapolate endpoints of tails\n",
    "alternative_bandwidth_prcntile =  False     ### Alternative bandwidth/percentile measures\n",
    "write_engelcurves              =  False     ### Whether to write Engel curves to intermediate file\n",
    "read_engelcurves_frm_file      =  True     ### Whether to read Engel curves directly from intermediate file (if True, doesn't recompute)\n",
    "compute_welfare                =  True      ### If false, only computes smoothed engel curves, not welfare measures\n",
    "write_output                   =  False     ### Whether to write welfare output to file or not\n",
    "write_stata                    =  False     ### Whether to write out to Stata or csv file (if False, writes to csv)\n",
    "delete_neg_exp_shares          =  False     ### If there are any negative expenditure shares at ends of curve, extrapolate\n",
    "weight_medians                 =  False     ### Whether to weight medians, which is pretty slow to run (this doesn't seem to work)\n",
    "sarhan_correction              =  True      ### Whether to use uniformity assumption for missing medians\n",
    "panel                          =  False     ### T/F for panel\n",
    "round_to_decile                =  False     ### For plotting welfare, rounds percentiles to nearest decile\n",
    "first_order_approx             =  False     ### First order approximation (not implented yet) (requires auxiliary data on prices)\n",
    "exact_price_correction         =  False     ### Exact price correction (requires auxiliary data on prices)\n",
    "\n",
    "### Variable names\n",
    "market_id                      =  \"market_id\"\n",
    "period_id                      =  \"period_id\"\n",
    "good_id                        =  \"i_good\"\n",
    "group_id                       =  \"G_group\"\n",
    "period_0                       =  43\n",
    "period_1                       =  55\n",
    "### Code assumes hh_id is unique across markets but not necessarily periods. User MUST check this\n",
    "hh_id                          =  \"hh_id\"\n",
    "hh_wt                          =  \"wt\"\n",
    "### outlays_var can be either a reference to a specific variable or generated from exp_var using \"Gen\"\n",
    "outlays_var                    =  \"exp_cap\"\n",
    "exp_var                        =  \"expenditure\"\n",
    "percentile                     =  \"percentile\"   ### This var created in situ \n",
    "d_price_var                    =  \"dp_prd1_prd0\" ### Only used with exact_price_correction == True\n",
    "type_extrap_tails              =  \"linear\"       ### can be linear, cubic, spline, first, second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read in data\n",
    "df = pd.read_stata(cons_data)\n",
    "\n",
    "### Save group-good matching\n",
    "group_df   = df[[good_id, group_id]].drop_duplicates().reset_index(drop=True)\n",
    "group_dict = dict(zip(group_df[good_id], group_df[group_id]))\n",
    "mkt_gd_df  = df[[market_id,good_id]].drop_duplicates().sort_values([market_id,good_id]).reset_index(drop=True)\n",
    "\n",
    "### Create evaluation point grid\n",
    "eval_grid = np.arange(0,1.0+1/evaluation_points,step=1.0/evaluation_points)\n",
    "eval_grid = np.array([np.round(a,15) for a in eval_grid])\n",
    "\n",
    "if not read_engelcurves_frm_file:\n",
    "    if outlays_var != \"Gen\":\n",
    "        hh_exp_df = df.groupby([market_id,period_id,hh_id,hh_wt])[outlays_var].apply(np.mean).reset_index()\n",
    "    else:\n",
    "        hh_exp_df              = df.groupby([market_id,period_id,hh_id,hh_wt])[exp_var].sum().reset_index()\n",
    "        hh_exp_df[outlays_var] = hh_exp_df[exp_var]\n",
    "        hh_exp_df.drop(exp_var,axis=1,inplace=True)\n",
    "        \n",
    "    if winsorize:\n",
    "        winsor_df = hh_exp_df.groupby([market_id,period_id])[outlays_var].apply(lambda x: stats.mstats.winsorize(x, limits=0.001)).reset_index()\n",
    "\n",
    "        winsorized_list = []\n",
    "        for a in list(winsor_df[outlays_var]):\n",
    "            for b in a:\n",
    "                winsorized_list.append(b)\n",
    "\n",
    "        hh_exp_df[outlays_var] = winsorized_list\n",
    "    ### Generate logged expenditure\n",
    "    hh_exp_df['logexp_cap'] = np.log(hh_exp_df[outlays_var])\n",
    "\n",
    "    ### Generate market by period dataframe\n",
    "    mkt_prd_df                       = hh_exp_df.groupby([market_id,period_id])['logexp_cap'].max(min_count=0).reset_index()\n",
    "    mkt_prd_df.rename(columns={'logexp_cap':'max_exp'},inplace=True)\n",
    "    mkt_prd_df['min_exp']            = hh_exp_df.groupby([market_id,period_id])['logexp_cap'].min(min_count=0).reset_index()['logexp_cap']\n",
    "    mkt_prd_df['wt_mkt_prd']         = hh_exp_df.groupby([market_id,period_id])[hh_wt].sum(min_count=0).reset_index()[hh_wt]\n",
    "    hh_exp_df['rank']                = hh_exp_df.groupby([market_id,period_id])[outlays_var].rank(method='first')\n",
    "    mkt_prd_df['num_households_mkt'] = hh_exp_df.groupby([market_id,period_id])[outlays_var].count().reset_index()[outlays_var]\n",
    "    mkt_prd_df['uniq_obs_outlays']   = hh_exp_df.groupby([market_id,period_id])[outlays_var].nunique().reset_index()[outlays_var]\n",
    "\n",
    "    hh_exp_df                        = hh_exp_df.merge(mkt_prd_df, on=[market_id,period_id], how='left')\n",
    "    hh_exp_df.sort_values([market_id, period_id],inplace=True)\n",
    "    hh_exp_df, mkt_prd_i             = create_identifier(hh_exp_df,[market_id,period_id],'mkt_prd',return_id_df=True)\n",
    "    hh_exp_df['bandwidth_rng']       = (hh_exp_df['max_exp']-hh_exp_df['min_exp'])*bandwidth_portion\n",
    "\n",
    "    ### Different ways to calculate which percentile a household falls into\n",
    "    if alternative_bandwidth_prcntile:\n",
    "        hh_exp_df['hh_prcnt_dist']   = (hh_exp_df['rank'].astype(float)-1)/(hh_exp_df['num_households_mkt']-1)\n",
    "        hh_exp_df['bw_mpce']         = 1.0*(1/(hh_exp_df['num_households_mkt']-1))*(hh_exp_df['num_households_mkt']/hh_exp_df['uniq_obs_outlays'])**3.0\n",
    "        hh_exp_df['bw_mpce']         = hh_exp_df.apply(lambda x: 1.0*(1/(evaluation_points-1))*(x['num_households_mkt']/x['uniq_obs_outlays'])**3.0 if evaluation_points/x['num_households_mkt'] > 1 else x['bw_mpce'], axis =1)\n",
    "    else:\n",
    "        hh_exp_df['hh_prcnt_dist']   = (hh_exp_df['rank'].astype(float)-0.5)/hh_exp_df['num_households_mkt']\n",
    "        hh_exp_df['bw_mpce']         = (evaluation_points+1)/(100.0*(hh_exp_df['num_households_mkt']-1))\n",
    "        \n",
    "    ### Generate smoothed income for each market-period\n",
    "    smoothed_inc = {}\n",
    "    for i, mkt_prd in enumerate(mkt_prd_i['mkt_prd']):\n",
    "        print(\"Running market-period:\", mkt_prd)\n",
    "        p = lpoly('hh_prcnt_dist', 'logexp_cap', eval_grid, 'bw_mpce', hh_exp_df[hh_exp_df['mkt_prd'] == mkt_prd], hh_wt)\n",
    "        smoothed_inc[(mkt_prd_i.loc[i,market_id],mkt_prd_i.loc[i,period_id])] = p\n",
    "\n",
    "    mkt_prd_good, good_cons_df = gen_good_cons_df(df=df, hh_exp_df=hh_exp_df, group_df=group_df, \n",
    "                                                hh_id=hh_id, period_id=period_id, market_id=market_id,\n",
    "                                                good_id=good_id,exp_var=exp_var,outlays_var=outlays_var, \n",
    "                                                hh_wt=hh_wt, period_0=period_0, period_1=period_1,\n",
    "                                                group_id = group_id)\n",
    "    \n",
    "    ### Generate smoothed expenditures for each market-period-good\n",
    "    smoothed_exp = {}\n",
    "    for i, mpg in enumerate(mkt_prd_good['mkt_prd_good']):\n",
    "        print(\"Running market-period-good:\", mpg)\n",
    "        mkt = mkt_prd_good.loc[i,market_id]\n",
    "        prd = mkt_prd_good.loc[i,period_id]\n",
    "        gd  = mkt_prd_good.loc[i,good_id]\n",
    "        tdf = good_cons_df[good_cons_df['mkt_prd_good'] == mpg]\n",
    "        smoothed_exp[(mkt,prd,gd)] = lpoly('logexp_cap', 'exp_share_g', smoothed_inc[(mkt, prd)], 'bandwidth_rng', tdf, hh_wt)\n",
    "    \n",
    "    smoothed_df = good_cons_df[[market_id,period_id,good_id,group_id,'num_households_mkt','wt_mkt_prd']].drop_duplicates()\n",
    "\n",
    "    smoothed_df = gen_comparision_df(smoothed_inc,smoothed_exp,smoothed_df,evl_grid=eval_grid, evl_points=evaluation_points,\n",
    "                                    period_id=period_id, market_id=market_id, good_id=good_id, period_0=period_0, period_1=period_1,\n",
    "                                    group_id=group_id)\n",
    "\n",
    "    ### For writing Engel curves to intermediate file\n",
    "    if write_engelcurves:      \n",
    "        smoothed_df.to_stata(smth_x_shares_dta, write_index=False)\n",
    "\n",
    "### Now compute welfare if requested\n",
    "if compute_welfare:\n",
    "    ### If reading Engel curves/smoothed income from intermediate file\n",
    "    if read_engelcurves_frm_file:\n",
    "        ### Read Engel curves from file \n",
    "        smoothed_df = pd.read_stata(smth_x_shares_dta)\n",
    "\n",
    "        smoothed_inc, smoothed_exp = dataframe_to_dict(smoothed_df, period_id=period_id, market_id=market_id, good_id=good_id,\n",
    "                                                                    period_0=period_0, period_1=period_1)\n",
    "    \n",
    "    if exact_price_correction:\n",
    "        price_df = pd.read_stata(price_data)\n",
    "        d_price_dict = dict(zip(list(zip(price_df[market_id],price_df[good_id])), price_df[d_price_var]))\n",
    "        sum_price_grp = {}\n",
    "        mkts_missing = []\n",
    "        for key in smoothed_exp.keys():\n",
    "            mkt, prd, gd = key\n",
    "            sum_price_grp[(mkt, prd, group_dict[gd])] = 0\n",
    "           \n",
    "            if not (mkt,gd) in d_price_dict.keys(): #### 10/04/23: Is sample different??!?\n",
    "                mkts_missing.append(mkt)\n",
    "                ### 10/09/23: replace with NAN if missing\n",
    "            else:\n",
    "                if prd == period_0:\n",
    "                    smoothed_exp[key]=smoothed_exp[key]*np.exp(-(sigma-1) * d_price_dict[mkt,gd])\n",
    "                elif prd == period_1:\n",
    "                    smoothed_exp[key]=smoothed_exp[key]*np.exp((sigma-1) * d_price_dict[mkt,gd])\n",
    "                else: \n",
    "                    raise MorethanTwoPeriods\n",
    "            sum_price_grp[(mkt, prd, group_dict[gd])] += smoothed_exp[key]\n",
    "        for key in smoothed_exp.keys():\n",
    "            mkt, prd, gd = key\n",
    "            smoothed_exp[key] = smoothed_exp[key]/sum_price_grp[(mkt, prd, group_dict[gd])]\n",
    "        mkts_missing = list(set(mkts_missing))\n",
    "        print(\"Missing markets:\", mkts_missing)\n",
    "        \n",
    "    print(\"checking monotonicity\")\n",
    "    monotonicity_dict = {}\n",
    "    for key in smoothed_exp.keys():\n",
    "        smoothed_exp[key] = monotonicity_tails(smoothed_exp[key], extrapolate_end=extrapolate_tails, \n",
    "                                                    evl_grid=eval_grid, evl_points=evaluation_points,\n",
    "                                                    type_extrapolation=type_extrap_tails)\n",
    "\n",
    "        if delete_neg_exp_shares:\n",
    "            smoothed_exp[key] = replace_neg_exp_shares(smoothed_exp[key], evl_grid=eval_grid)\n",
    "        monotonicity_dict[key] = monotonicity_check(smoothed_exp[key])\n",
    "\n",
    "    print(\"identifying horizontal shifts\")\n",
    "    if panel:\n",
    "        yh_df, p0_in_p1, p1_in_p0, use_curves, num_useable_goods_group = identify_horizontal_shifts_panel(smoothed_exp,\n",
    "                                   smoothed_inc_dict=smoothed_inc, monotonicity_dict=monotonicity_dict,\n",
    "                                   good_cons_df=good_cons_df,group_dict=group_dict, evl_points=evaluation_points,\n",
    "                                   hh_id=hh_id, market_id=market_id, good_id=good_id, group_id=group_id, period_id=period_id, \n",
    "                                   period_0=period_0, period_1=period_1)\n",
    "    else:\n",
    "        yh0, yh1, p0_in_p1, p1_in_p0, use_curves, num_useable_goods_group = identify_horizontal_shifts(smoothed_exp,\n",
    "                                   smoothed_inc_dict=smoothed_inc, monotonicity_dict=monotonicity_dict,\n",
    "                                   mkt_gd_df=mkt_gd_df,group_dict=group_dict, evl_points=evaluation_points,\n",
    "                                   market_id=market_id, good_id=good_id, period_0=period_0, period_1=period_1)\n",
    "        \n",
    "    print(\"welfare df\")\n",
    "    if panel:\n",
    "        welfare_df = gen_welfare_df(smoothed_inc, smoothed_exp,smoothed_df=yh_df, \n",
    "                                yh0_dict={},yh1_dict={}, p0_in_p1_dict=p0_in_p1,\n",
    "                                p1_in_p0_dict=p1_in_p0, use_curves_dict=use_curves,\n",
    "                                num_gds_dict=num_useable_goods_group, \n",
    "                                monotonicity_dict=monotonicity_dict,\n",
    "                                evl_grid=eval_grid, evl_points=evaluation_points,\n",
    "                                hh_id=hh_id,market_id=market_id,good_id=good_id,group_id=group_id,\n",
    "                                period_id=period_id,period_0=period_0, period_1=period_1, \n",
    "                                panel=panel)\n",
    "    else:\n",
    "        welfare_df = gen_welfare_df(smoothed_inc, smoothed_exp,smoothed_df=smoothed_df, \n",
    "                                yh0_dict=yh0,yh1_dict=yh1, p0_in_p1_dict=p0_in_p1,\n",
    "                                p1_in_p0_dict=p1_in_p0, use_curves_dict=use_curves,\n",
    "                                num_gds_dict=num_useable_goods_group, \n",
    "                                monotonicity_dict=monotonicity_dict,\n",
    "                                evl_grid=eval_grid, evl_points=evaluation_points,\n",
    "                                hh_id=hh_id,market_id=market_id,good_id=good_id,group_id=group_id,\n",
    "                                period_id=period_id,period_0=period_0, period_1=period_1, \n",
    "                                panel=panel)\n",
    "\n",
    "    print(\"p0 and p1\")\n",
    "    ### Construct P0 and P1\n",
    "    if panel:\n",
    "        engel_outlays_var0, engel_outlays_var1 = 'logexp_cap0', 'logexp_cap1'\n",
    "        wts_prd0, wts_prd1 = hh_wt+'0', hh_wt+'1'\n",
    "        groupby_vars = [market_id, hh_id]\n",
    "    else:\n",
    "        engel_outlays_var0, engel_outlays_var1 = 'log_smoothed_outlays0', 'log_smoothed_outlays1'\n",
    "        wts_prd0, wts_prd1 = 'wt_mkt_prd0', 'wt_mkt_prd1'\n",
    "        groupby_vars = [market_id, percentile]\n",
    "\n",
    "    welfare_df['wt_mkt_prd']      = welfare_df[wts_prd0]+welfare_df[wts_prd1]\n",
    "    welfare_df['logP0']           = welfare_df['yh1']-welfare_df[engel_outlays_var0]\n",
    "    # welfare_df['logP0_nonmon']    = welfare_df['logP0']\n",
    "    welfare_df['logP0']           = welfare_df.apply(lambda x: np.nan if x['use_curves'] == 0 else x['logP0'], axis=1)\n",
    "    welfare_df['logP0_ranked']    = welfare_df['logP0']\n",
    "    welfare_df['logP0']           = welfare_df['logP0'].apply(lambda x: np.nan if np.isinf(x) else x)\n",
    "\n",
    "    welfare_df['logP1']           = welfare_df['yh0']-welfare_df[engel_outlays_var1]\n",
    "    # welfare_df['logP1_nonmon']    = welfare_df['logP1']\n",
    "    welfare_df['logP1']           = welfare_df.apply(lambda x: np.nan if x['use_curves'] == 0 else x['logP1'], axis=1)\n",
    "    welfare_df['logP1_ranked']    = -1.0 * welfare_df['logP1']\n",
    "    welfare_df['logP1']           = welfare_df['logP1'].apply(lambda x: np.nan if np.isinf(x) else x)\n",
    "    welfare_df['logP1_neg']       = -1.0 * welfare_df['logP1']\n",
    "\n",
    "    minmaxdf                      = welfare_df.groupby(groupby_vars)[['logP0','logP1_neg']].agg(['min','max']).reset_index()\n",
    "    minmaxdf.columns              = groupby_vars+['minlogP0','maxlogP0','minlogP1','maxlogP1']\n",
    "    welfare_df                    = welfare_df.merge(minmaxdf, on=groupby_vars, how='left')\n",
    "\n",
    "    ### Identify noncrossing points and rank accordingly\n",
    "    print(\"identify noncrossings\")\n",
    "    welfare_df['logP0_ranked'] = welfare_df.apply(identify_non_crossings, p0_or_p1='P0', axis=1)\n",
    "    welfare_df['logP1_ranked'] = welfare_df.apply(identify_non_crossings, p0_or_p1='P1', axis=1)\n",
    "\n",
    "    ### Compute first-order price correction here\n",
    "    first_order_corr_p0 = 0\n",
    "    first_order_corr_p1 = 0\n",
    "\n",
    "    ### Take medians\n",
    "    print(\"medians\")\n",
    "    grouped_medians_maxes         = welfare_df.groupby(groupby_vars)[['logP0_ranked','logP1_ranked']].median().reset_index()\n",
    "    grouped_medians_maxes.columns = groupby_vars+['logP0_med','logP1_med']\n",
    "    ### Weighted medians\n",
    "    print(\"weighted medians\")\n",
    "    if weight_medians:\n",
    "        wght_med_p0_df                = welfare_df.groupby(groupby_vars).apply(weighted_median, val='logP0_ranked', weight='wt_mkt_prd', dropna=True).reset_index()\n",
    "        wght_med_p0_df.columns        = groupby_vars+['logP0_wmed']\n",
    "        wght_med_p1_df                = welfare_df.groupby(groupby_vars).apply(weighted_median, val='logP1_ranked', weight='wt_mkt_prd', dropna=True).reset_index()\n",
    "        wght_med_p1_df.columns        = groupby_vars+['logP1_wmed']\n",
    "    ### Weighted means\n",
    "    print(\"weighted means\")\n",
    "    wght_avg_df = welfare_df.groupby(groupby_vars).apply(lambda g: pd.Series({\n",
    "        'logP0_wtmean':               nan_wght_average(g['logP0'],    weights=g['wt_mkt_prd']),\n",
    "        'logP1_wtmean':               nan_wght_average(g['logP1_neg'],    weights=g['wt_mkt_prd']),\n",
    "        })).reset_index()\n",
    "\n",
    "    ### Merge various meds, means together\n",
    "    grouped_medians_maxes         = grouped_medians_maxes.merge(minmaxdf, on=groupby_vars, how='outer')\n",
    "    if weight_medians:\n",
    "        grouped_medians_maxes         = grouped_medians_maxes.merge(wght_med_p0_df, on=groupby_vars, how='outer')\n",
    "        grouped_medians_maxes         = grouped_medians_maxes.merge(wght_med_p1_df, on=groupby_vars, how='outer')\n",
    "    grouped_medians_maxes         = grouped_medians_maxes.merge(wght_avg_df, on=groupby_vars, how='outer')\n",
    "\n",
    "    ### Replace medians with NaNs if they are outside the min/max range\n",
    "    grouped_medians_maxes['logP0_med']  = grouped_medians_maxes.apply(lambda x: np.nan if (x['logP0_med'] < x['minlogP0'] or x['logP0_med'] > x['maxlogP0']) else x['logP0_med'], axis = 1)\n",
    "    grouped_medians_maxes['logP1_med']  = grouped_medians_maxes.apply(lambda x: np.nan if (x['logP1_med'] < x['minlogP1'] or x['logP1_med'] > x['maxlogP1']) else x['logP1_med'], axis = 1)\n",
    "    if weight_medians:\n",
    "        grouped_medians_maxes['logP0_wmed'] = grouped_medians_maxes.apply(lambda x: np.nan if (x['logP0_wmed'] < x['minlogP0'] or x['logP0_wmed'] > x['maxlogP0']) else x['logP0_wmed'], axis = 1)\n",
    "        grouped_medians_maxes['logP1_wmed'] = grouped_medians_maxes.apply(lambda x: np.nan if (x['logP1_wmed'] < x['minlogP1'] or x['logP1_wmed'] > x['maxlogP1']) else x['logP1_wmed'], axis = 1)\n",
    "    welfare_df   = welfare_df.merge(grouped_medians_maxes.drop(['minlogP0','maxlogP0','minlogP1','maxlogP1'], axis=1), on=groupby_vars, how='left')\n",
    "\n",
    "    print(\"sarhan correction\")\n",
    "    ### perform sarhan correction\n",
    "    if sarhan_correction:\n",
    "        welfare_df['n_0']     = welfare_df['use_curves']\n",
    "        welfare_df['n_1']     = welfare_df['use_curves']\n",
    "        welfare_df['r1_0']    = welfare_df.apply(lambda x: 0 if x['logP0_ranked'] >= x['minlogP0'] else x['use_curves'],axis=1)\n",
    "        welfare_df['r2_0']    = welfare_df.apply(lambda x: 0 if x['logP0_ranked'] <= x['maxlogP0'] else x['use_curves'],axis=1)\n",
    "        welfare_df['r1_1']    = welfare_df.apply(lambda x: 0 if x['logP1_ranked'] >= x['minlogP1'] else x['use_curves'],axis=1)\n",
    "        welfare_df['r2_1']    = welfare_df.apply(lambda x: 0 if x['logP1_ranked'] <= x['maxlogP1'] else x['use_curves'],axis=1)\n",
    "\n",
    "        sarhan_df             = welfare_df.groupby(groupby_vars)[['n_0','n_1','r1_0','r2_0', 'r1_1','r2_1']].sum().reset_index()\n",
    "        sarhan_df             = sarhan_df.merge(grouped_medians_maxes[groupby_vars+['minlogP0','maxlogP0','minlogP1','maxlogP1']], on=groupby_vars, how='left')\n",
    "        sarhan_df['logPO_sc'] = (1/(2*(sarhan_df['n_0']-sarhan_df['r1_0']-sarhan_df['r2_0']-1)))*(((sarhan_df['n_0']-2*sarhan_df['r2_0']-1)*sarhan_df['minlogP0'])+((sarhan_df['n_0']-2*sarhan_df['r1_0']-1)*sarhan_df['maxlogP0']))\n",
    "        sarhan_df['logP1_sc'] = (1/(2*(sarhan_df['n_1']-sarhan_df['r1_1']-sarhan_df['r2_1']-1)))*(((sarhan_df['n_1']-2*sarhan_df['r2_1']-1)*sarhan_df['minlogP1'])+((sarhan_df['n_1']-2*sarhan_df['r1_1']-1)*sarhan_df['maxlogP1']))\n",
    "        welfare_df.drop(['n_0','n_1','r1_0','r2_0','r1_1','r2_1'],axis=1,inplace=True)\n",
    "\n",
    "        welfare_df                 = welfare_df.merge(sarhan_df[groupby_vars+['logPO_sc','logP1_sc']], on=groupby_vars, how='left')\n",
    "        welfare_df['logP0_med_sc'] = welfare_df.apply(lambda x: x['logPO_sc'] if np.isnan(x['logP0_med']) else x['logP0_med'],axis=1)\n",
    "        welfare_df['logP1_med_sc'] = welfare_df.apply(lambda x: x['logP1_sc'] if np.isnan(x['logP1_med']) else x['logP1_med'],axis=1)\n",
    "        if weight_medians:\n",
    "            welfare_df['logP0_wmed_sc'] = welfare_df.apply(lambda x: x['logPO_sc'] if np.isnan(x['logP0_wmed']) else x['logP0_wmed'],axis=1)\n",
    "            welfare_df['logP1_wmed_sc'] = welfare_df.apply(lambda x: x['logP1_sc'] if np.isnan(x['logP1_wmed']) else x['logP1_wmed'],axis=1)\n",
    "\n",
    "    print(\"writing to file\")\n",
    "    if write_output:\n",
    "        if write_stata:\n",
    "            ### Write to stata dta\n",
    "            welfare_df_stata        = welfare_df.copy()\n",
    "            welfare_df_stata['yh0'] = welfare_df_stata['yh0'].apply(lambda x: infinity_stata if np.isposinf(x) else x)\n",
    "            welfare_df_stata['yh0'] = welfare_df_stata['yh0'].apply(lambda x: -infinity_stata if np.isneginf(x) else x)\n",
    "            welfare_df_stata['yh1'] = welfare_df_stata['yh1'].apply(lambda x: infinity_stata if np.isposinf(x) else x)\n",
    "            welfare_df_stata['yh1'] = welfare_df_stata['yh1'].apply(lambda x: -infinity_stata if np.isneginf(x) else x)\n",
    "            welfare_df_stata.to_stata(price_indices_dta, write_index=False)\n",
    "        else:\n",
    "            ### Write to csv \n",
    "            welfare_df.to_csv(price_indices_dta.replace(\".dta\",\".csv\"), index=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot Engel curves\n",
    "if panel:\n",
    "    cols_0 = ['bandwidth_rng0','count0','exp_G0','exp_cap0','exp_share_g0','expenditure0','logexp_cap0','num_households_mkt0',\n",
    "          'wt0','wt_mkt_prd0','yh1','p1_in_p0','use_curves','curve_mon1','num_useable_goods_group','mkt_good', 'percentile0',\n",
    "          'logP0', 'logP0_ranked', 'minlogP0', 'maxlogP0', 'logP0_med', 'logP0_wmed', \n",
    "          'logP0_wtmean', 'logPO_sc', 'logP0_med_sc', 'logP0_wmed_sc']\n",
    "    cols_1 = ['bandwidth_rng1', 'count1', 'exp_G1', 'exp_cap1', 'exp_share_g1', 'expenditure1', 'logexp_cap1', 'num_households_mkt1',\n",
    "            'wt1', 'wt_mkt_prd1', 'yh0', 'p0_in_p1', 'use_curves', 'curve_mon0', 'num_useable_goods_group', 'mkt_good', 'percentile1', \n",
    "            'logP1',  'logP1_ranked',  'minlogP1',  'maxlogP1',  'logP1_med',  'logP1_wmed', \n",
    "            'logP1_wtmean',  'logPO_sc',  'logP1_med_sc',  'logP1_wmed_sc']\n",
    "\n",
    "    # Separate the DataFrame based on periods:\n",
    "    wel_df_0 = welfare_df[[hh_id, good_id, market_id, group_id] + cols_0].copy()\n",
    "    wel_df_1 = welfare_df[[hh_id, good_id, market_id, group_id] + cols_1].copy()\n",
    "\n",
    "    # Rename columns to drop the period suffix:\n",
    "    wel_df_0.columns = [col[:-1] if ((col[-1:] in [\"0\",\"1\"]) and (col[-2:-1] != 'P')) else col for col in wel_df_0.columns]\n",
    "    wel_df_0.rename(columns={'yh':'yh1','p1_in_p':'p1_in_p0','wt_mkt_prd':'wt_mkt_prd0'},inplace=True)\n",
    "    wel_df_1.columns = [col[:-1] if ((col[-1:] in [\"0\",\"1\"]) and (col[-2:-1] != 'P')) else col for col in wel_df_1.columns]\n",
    "    wel_df_1.rename(columns={'yh':'yh0','p0_in_p':'p0_in_p1','wt_mkt_prd':'wt_mkt_prd1'},inplace=True)\n",
    "\n",
    "    # Add the period column:\n",
    "    wel_df_0[period_id] = 0\n",
    "    wel_df_1[period_id] = 1\n",
    "\n",
    "    # Concatenate the dataframes for the two periods:\n",
    "    welfare_plot_df = pd.concat([wel_df_0.reset_index(drop=True), wel_df_1.reset_index(drop=True)])\n",
    "    welfare_plot_df['wt_mkt_prd'] = welfare_plot_df['wt_mkt_prd0'].fillna(0) + welfare_plot_df['wt_mkt_prd1'].fillna(0)\n",
    "else:\n",
    "    welfare_plot_df = welfare_df.copy()\n",
    "\n",
    "if round_to_decile:\n",
    "    welfare_plot_df['decile']   = (welfare_plot_df[percentile].round(1))*100\n",
    "else:\n",
    "    welfare_plot_df['decile']   = (welfare_plot_df[percentile].round(2))*100\n",
    "\n",
    "if not sarhan_correction:\n",
    "    welfare_plot_df['logP0_med_sc']      =  np.NaN\n",
    "    welfare_plot_df['logP1_med_sc']      =  np.NaN\n",
    "    if not weight_medians:\n",
    "        welfare_plot_df['logP0_wmed_sc'] =  np.NaN\n",
    "        welfare_plot_df['logP1_wmed_sc'] =  np.NaN\n",
    "if not weight_medians:\n",
    "    welfare_plot_df['logP0_wmed']        =  np.NaN\n",
    "    welfare_plot_df['logP1_wmed']        =  np.NaN\n",
    "    welfare_plot_df['logP0_wmed_sc']     =  np.NaN\n",
    "    welfare_plot_df['logP1_wmed_sc']     =  np.NaN\n",
    "\n",
    "welfare_plot_df = welfare_plot_df.groupby('decile').apply(lambda g: pd.Series({\n",
    "    'logP0':               nan_wght_average(g['logP0_med'],    weights=g['wt_mkt_prd']),\n",
    "    'logP1':               nan_wght_average(g['logP1_med'],    weights=g['wt_mkt_prd']),\n",
    "    'logP0_sc':            nan_wght_average(g['logP0_med_sc'], weights=g['wt_mkt_prd']),\n",
    "    'logP1_sc':            nan_wght_average(g['logP1_med_sc'], weights=g['wt_mkt_prd']),\n",
    "    'logP0_wmed':          nan_wght_average(g['logP0_wmed'],   weights=g['wt_mkt_prd']),\n",
    "    'logP1_wmed':          nan_wght_average(g['logP1_wmed'],   weights=g['wt_mkt_prd']),\n",
    "    'logP0_wmed_sc':       nan_wght_average(g['logP0_wmed_sc'],weights=g['wt_mkt_prd']),\n",
    "    'logP1_wmed_sc':       nan_wght_average(g['logP1_wmed_sc'],weights=g['wt_mkt_prd']),\n",
    "    'logP0_wtmean':        nan_wght_average(g['logP0_wtmean'], weights=g['wt_mkt_prd']),\n",
    "    'logP1_wtmean':        nan_wght_average(g['logP1_wtmean'], weights=g['wt_mkt_prd']),\n",
    "})).reset_index()\n",
    "\n",
    "log_variables = [\n",
    "    \"logP0\",\"logP1\",\"logP0_wtmean\",\"logP1_wtmean\"\n",
    "]\n",
    "\n",
    "if sarhan_correction and weight_medians:\n",
    "    log_variables = log_variables + [\"logP0_sc\",\"logP1_sc\",\"logP0_wmed\",\"logP1_wmed\",\"logP0_wmed_sc\",\"logP1_wmed_sc\"]\n",
    "elif sarhan_correction and not weight_medians:\n",
    "    log_variables = log_variables + [\"logP0_sc\",\"logP1_sc\"]\n",
    "elif not sarhan_correction and weight_medians:\n",
    "    log_variables = log_variables + [\"logP0_wmed\",\"logP1_wmed\"]\n",
    "\n",
    "for log_var in log_variables:\n",
    "    nonlogvar = log_var[3:]\n",
    "    welfare_plot_df['pc_' + nonlogvar] = 100 * (np.exp(welfare_plot_df[log_var]) - 1) ### percent change\n",
    "\n",
    "lastpart = \"\"\n",
    "if exact_price_correction:\n",
    "    lastpart = \"_exact_price\"\n",
    "\n",
    "plot_bars(welfare_plot_df, 'pc_P1',        'pc_P0',         'No Good-Level Correction, simple median', filename=os.path.join(plot_dir, \"wf_med_\"+type_extrap_tails+\"_myengels\"+lastpart+\".pdf\"))\n",
    "plot_bars(welfare_plot_df, 'pc_P1_wtmean',        'pc_P0_wtmean',         'No Good-Level Correction, weighted average', filename=os.path.join(plot_dir, \"wf_wtmean_\"+type_extrap_tails+\"_myengels\"+lastpart+\".pdf\"))\n",
    "if sarhan_correction:\n",
    "    plot_bars(welfare_plot_df, 'pc_P1_sc',         'pc_P0_sc',         'Sarhan Uniformity Correction', filename=os.path.join(plot_dir, \"wf_sc_\"+type_extrap_tails+\"_myengels\"+lastpart+\".pdf\"))\n",
    "if weight_medians:\n",
    "    plot_bars(welfare_plot_df, 'pc_P1_wmed',        'pc_P0_wmed',        'No Good-Level Correction, weighted median')\n",
    "if sarhan_correction and weight_medians:\n",
    "    plot_bars(welfare_plot_df, 'pc_P1_wmed_sc',        'pc_P0_wmed_sc',        'Sarhan Uniformity Correction, weighted median')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a77d8c3dd7e6e972e019be1540f4ebd7fa9e3c66783be27443f83372c691781e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
